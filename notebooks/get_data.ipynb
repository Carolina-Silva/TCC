{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8598e281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Collecting pysus\n",
      "  Downloading pysus-1.0.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/carolina/Documents/TCC/TCC/.conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting Unidecode<2.0.0,>=1.3.6 (from pysus)\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aioftp<0.22.0,>=0.21.4 (from pysus)\n",
      "  Downloading aioftp-0.21.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting bigtree<0.13.0,>=0.12.2 (from pysus)\n",
      "  Downloading bigtree-0.12.5-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting dateparser<2.0.0,>=1.1.8 (from pysus)\n",
      "  Downloading dateparser-1.3.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting dbfread==2.0.7 (from pysus)\n",
      "  Downloading dbfread-2.0.7-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting elasticsearch==7.16.2 (from elasticsearch[preprocessing]==7.16.2->pysus)\n",
      "  Downloading elasticsearch-7.16.2-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting fastparquet<=2024.11.0,>=2023.10.1 (from pysus)\n",
      "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting humanize<5.0.0,>=4.8.0 (from pysus)\n",
      "  Downloading humanize-4.15.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting loguru<0.7.0,>=0.6.0 (from pysus)\n",
      "  Downloading loguru-0.6.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting pyarrow>=11.0.0 (from pysus)\n",
      "  Downloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pycparser==2.21 (from pysus)\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyreaddbc>=1.1.0 (from pysus)\n",
      "  Downloading pyreaddbc-1.2.0-cp311-cp311-manylinux_2_37_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting tqdm==4.64.0 (from pysus)\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.9.0 in /home/carolina/Documents/TCC/TCC/.conda/lib/python3.11/site-packages (from pysus) (4.15.0)\n",
      "Collecting urwid<3.0.0,>=2.1.2 (from pysus)\n",
      "  Downloading urwid-2.6.16-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting wget<4.0,>=3.2 (from pysus)\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting urllib3<2,>=1.21.1 (from elasticsearch==7.16.2->elasticsearch[preprocessing]==7.16.2->pysus)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Collecting certifi (from elasticsearch==7.16.2->elasticsearch[preprocessing]==7.16.2->pysus)\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "\u001b[33mWARNING: elasticsearch 7.16.2 does not provide the extra 'preprocessing'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: six>=1.5 in /home/carolina/Documents/TCC/TCC/.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting regex>=2024.9.11 (from dateparser<2.0.0,>=1.1.8->pysus)\n",
      "  Downloading regex-2026.1.15-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tzlocal>=0.2 (from dateparser<2.0.0,>=1.1.8->pysus)\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting cramjam>=2.3 (from fastparquet<=2024.11.0,>=2023.10.1->pysus)\n",
      "  Downloading cramjam-2.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting fsspec (from fastparquet<=2024.11.0,>=2023.10.1->pysus)\n",
      "  Downloading fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging in /home/carolina/Documents/TCC/TCC/.conda/lib/python3.11/site-packages (from fastparquet<=2024.11.0,>=2023.10.1->pysus) (25.0)\n",
      "Requirement already satisfied: wcwidth in /home/carolina/Documents/TCC/TCC/.conda/lib/python3.11/site-packages (from urwid<3.0.0,>=2.1.2->pysus) (0.6.0)\n",
      "Collecting cffi<2,>=1.15.1 (from pyreaddbc>=1.1.0->pysus)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pyyaml>=6 (from pyreaddbc>=1.1.0->pysus)\n",
      "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Downloading pysus-1.0.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dbfread-2.0.7-py2.py3-none-any.whl (20 kB)\n",
      "Downloading elasticsearch-7.16.2-py2.py3-none-any.whl (385 kB)\n",
      "Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading aioftp-0.21.4-py3-none-any.whl (37 kB)\n",
      "Downloading bigtree-0.12.5-py3-none-any.whl (63 kB)\n",
      "Downloading dateparser-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanize-4.15.0-py3-none-any.whl (132 kB)\n",
      "Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "Downloading numpy-2.4.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading urwid-2.6.16-py3-none-any.whl (297 kB)\n",
      "Downloading cramjam-2.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyreaddbc-1.2.0-cp311-cp311-manylinux_2_37_x86_64.whl (64 kB)\n",
      "Downloading cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (467 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Downloading fsspec-2026.2.0-py3-none-any.whl (202 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9686 sha256=8b3b63e53535b26739851f9bd0ef5fa0f0251cb203c17573fb123f0b2ead5d28\n",
      "  Stored in directory: /home/carolina/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, pytz, dbfread, urwid, urllib3, Unidecode, tzlocal, tzdata, tqdm, regex, pyyaml, python-dateutil, pycparser, pyarrow, numpy, loguru, humanize, fsspec, cramjam, certifi, bigtree, aioftp, pandas, elasticsearch, dateparser, cffi, pyreaddbc, fastparquet, pysus\n",
      "\u001b[2K  Attempting uninstall: python-dateutil[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/29\u001b[0m [pyyaml]de]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post08;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/29\u001b[0m [pyyaml]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/29\u001b[0m [pyyaml]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/29\u001b[0m [pyyaml]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/29\u001b[0m [pysus]38;5;237m━\u001b[0m \u001b[32m28/29\u001b[0m [pysus]w]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Unidecode-1.4.0 aioftp-0.21.4 bigtree-0.12.5 certifi-2026.1.4 cffi-1.17.1 cramjam-2.11.0 dateparser-1.3.0 dbfread-2.0.7 elasticsearch-7.16.2 fastparquet-2024.11.0 fsspec-2026.2.0 humanize-4.15.0 loguru-0.6.0 numpy-2.4.2 pandas-2.3.3 pyarrow-23.0.0 pycparser-2.21 pyreaddbc-1.2.0 pysus-1.0.0 python-dateutil-2.8.2 pytz-2025.2 pyyaml-6.0.3 regex-2026.1.15 tqdm-4.64.0 tzdata-2025.3 tzlocal-5.3.1 urllib3-1.26.20 urwid-2.6.16 wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas pysus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fff37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ftplib\n",
    "import pandas as pd\n",
    "import pyreaddbc\n",
    "from dbfread import DBF\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4251bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando conexão com ftp.datasus.gov.br...\n",
      "Baixando DOSP2023.dbc...\n",
      "Download concluído.\n",
      "Iniciando conversão para CSV...\n",
      "Sucesso! Arquivo salvo em: /home/carolina/Documents/TCC/TCC/notebooks/dados_processados/DOSP2023.csv\n",
      "Total de registros processados: 334303\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        script_dir = os.getcwd()\n",
    "\n",
    "    raw_dir = os.path.join(script_dir, \"dados\")\n",
    "    processed_dir = os.path.join(script_dir, \"dados_processados\")\n",
    "\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    ftp_host = \"ftp.datasus.gov.br\"\n",
    "    ftp_path = \"/dissemin/publicos/SIM/CID10/DORES/\"\n",
    "    filename = \"DOSP2023.dbc\"\n",
    "\n",
    "    local_dbc = os.path.join(raw_dir, filename)\n",
    "    local_dbf = os.path.join(raw_dir, filename.replace('.dbc', '.dbf'))\n",
    "    local_csv = os.path.join(processed_dir, filename.replace('.dbc', '.csv'))\n",
    "\n",
    "    print(f\"Iniciando conexão com {ftp_host}...\")\n",
    "\n",
    "    try:\n",
    "        ftp = ftplib.FTP(ftp_host)\n",
    "        ftp.login()\n",
    "        ftp.cwd(ftp_path)\n",
    "\n",
    "        print(f\"Baixando {filename}...\")\n",
    "\n",
    "        with open(local_dbc, 'wb') as f:\n",
    "            ftp.retrbinary(f\"RETR {filename}\", f.write)\n",
    "\n",
    "        ftp.quit()\n",
    "        print(\"Download concluído.\")\n",
    "\n",
    "    except ftplib.all_errors as e:\n",
    "        print(f\"Erro no FTP: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Iniciando conversão para CSV...\")\n",
    "\n",
    "    try:\n",
    "        pyreaddbc.dbc2dbf(local_dbc, local_dbf)\n",
    "\n",
    "        table = DBF(local_dbf, encoding='iso-8859-1')\n",
    "        df = pd.DataFrame(iter(table))\n",
    "\n",
    "        df.to_csv(local_csv, index=False, encoding='utf-8')\n",
    "\n",
    "        if os.path.exists(local_dbf):\n",
    "            os.remove(local_dbf)\n",
    "\n",
    "        print(f\"Sucesso! Arquivo salvo em: {local_csv}\")\n",
    "        print(f\"Total de registros processados: {len(df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na conversão: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd3883b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded from: /home/carolina/Documents/TCC/TCC/notebooks/dados_processados/DOSP2023.csv\n",
      "Total records: 334303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255793/1404854105.py:2: DtypeWarning: Columns (66) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGEM</th>\n",
       "      <th>TIPOBITO</th>\n",
       "      <th>DTOBITO</th>\n",
       "      <th>HORAOBITO</th>\n",
       "      <th>NATURAL</th>\n",
       "      <th>CODMUNNATU</th>\n",
       "      <th>DTNASC</th>\n",
       "      <th>IDADE</th>\n",
       "      <th>SEXO</th>\n",
       "      <th>RACACOR</th>\n",
       "      <th>...</th>\n",
       "      <th>FONTES</th>\n",
       "      <th>TPRESGINFO</th>\n",
       "      <th>TPNIVELINV</th>\n",
       "      <th>NUDIASINF</th>\n",
       "      <th>DTCADINF</th>\n",
       "      <th>MORTEPARTO</th>\n",
       "      <th>DTCONCASO</th>\n",
       "      <th>FONTESINF</th>\n",
       "      <th>ALTCAUSA</th>\n",
       "      <th>CONTADOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1012023</td>\n",
       "      <td>720.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>351492.0</td>\n",
       "      <td>8011945.0</td>\n",
       "      <td>477</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1012023</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>822.0</td>\n",
       "      <td>220010.0</td>\n",
       "      <td>2091979.0</td>\n",
       "      <td>443</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1012023</td>\n",
       "      <td>2150.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>350400.0</td>\n",
       "      <td>17122022.0</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>SXXXXX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022023.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022023.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1072023</td>\n",
       "      <td>815.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>410050.0</td>\n",
       "      <td>7121989.0</td>\n",
       "      <td>433</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1072023</td>\n",
       "      <td>1935.0</td>\n",
       "      <td>825.0</td>\n",
       "      <td>251660.0</td>\n",
       "      <td>6021942.0</td>\n",
       "      <td>481</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ORIGEM  TIPOBITO  DTOBITO  HORAOBITO  NATURAL  CODMUNNATU      DTNASC  \\\n",
       "0       1         2  1012023      720.0    835.0    351492.0   8011945.0   \n",
       "1       1         2  1012023     2300.0    822.0    220010.0   2091979.0   \n",
       "2       1         2  1012023     2150.0    835.0    350400.0  17122022.0   \n",
       "3       1         2  1072023      815.0    841.0    410050.0   7121989.0   \n",
       "4       1         2  1072023     1935.0    825.0    251660.0   6021942.0   \n",
       "\n",
       "   IDADE  SEXO  RACACOR  ...  FONTES  TPRESGINFO  TPNIVELINV  NUDIASINF  \\\n",
       "0    477     2      1.0  ...     NaN         NaN         NaN        NaN   \n",
       "1    443     1      1.0  ...     NaN         NaN         NaN        NaN   \n",
       "2    215     1      1.0  ...  SXXXXX         NaN         NaN        NaN   \n",
       "3    433     1      1.0  ...     NaN         NaN         NaN        NaN   \n",
       "4    481     2      1.0  ...     NaN         NaN         NaN        NaN   \n",
       "\n",
       "    DTCADINF  MORTEPARTO  DTCONCASO  FONTESINF  ALTCAUSA  CONTADOR  \n",
       "0        NaN         NaN        NaN        NaN       NaN         2  \n",
       "1        NaN         NaN        NaN        NaN       NaN         3  \n",
       "2  2022023.0         3.0  2022023.0        NaN       2.0         5  \n",
       "3        NaN         NaN        NaN        NaN       NaN         6  \n",
       "4        NaN         NaN        NaN        NaN       NaN         7  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/home/carolina/Documents/TCC/TCC/notebooks/dados_processados/DOSP2023.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"DataFrame loaded from: {file_path}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6036304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_values(column):\n",
    "    null_count = df[column].isnull().sum()\n",
    "    return null_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e301fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'TPOBITOCOR'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMissing values per column:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# for col, count in missing_values.items():\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#     print(f\"{col}: {count}\")   \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mmissing_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTPOBITOCOR\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'TPOBITOCOR'"
     ]
    }
   ],
   "source": [
    "all_columns= df.columns\n",
    "missing_values = {col: count_missing_values(col) for col in all_columns}\n",
    "print(\"Missing values per column:\")\n",
    "\n",
    "\n",
    "# for col, count in missing_values.items():\n",
    "#     print(f\"{col}: {count}\")   \n",
    "\n",
    "\n",
    "missing_values.TPOBITOCOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52b9e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo para limpar e agrupar\n",
    "df['CID_LIMPO'] = df['LINHAA'].str.replace('*', '', regex=False)\n",
    "df['GRUPO_CID'] = df['CID_LIMPO'].str[:3] # Pega os 3 primeiros caracteres (ex: I21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4d3fc",
   "metadata": {},
   "source": [
    "1. Limpeza e Padronização\n",
    "Os dados brutos do SIH vêm com \"sujeiras\" de sistema que dificultam a análise.\n",
    "Remover caracteres especiais: Como você notou, os códigos vêm com um asterisco (*). O primeiro passo é removê-lo para que o código fique apenas como I119.\n",
    "Tratar valores vazios (NaN): Muitas linhas podem não ter diagnóstico secundário. Você precisa decidir se vai excluir essas linhas ou preenchê-las com algo como \"Não informado\" para não dar erro nos cálculos.\n",
    "Ajustar o tamanho do código: Às vezes o CID tem 3 caracteres, às vezes 4. Padronize todos para o mesmo formato para conseguir cruzar com a tabela de nomes.\n",
    "2. Tradução (O \"De-Para\")\n",
    "Ter apenas I219 não ajuda muito na leitura do relatório.\n",
    "Você deve pegar uma tabela de referência (o dicionário da CID-10) e dizer ao computador: \"Sempre que encontrar I219, escreva ao lado 'Infarto agudo do miocárdio'\". Isso torna seus gráficos e tabelas compreensíveis para qualquer pessoa.\n",
    "3. Agrupamento por Categorias\n",
    "Analisar código por código (ex: I210, I211, I212) gera tabelas gigantescas e confusas.\n",
    "Criar Grupos: O ideal é agrupar. Em vez de 50 tipos de infarto, você cria uma categoria única chamada \"Doenças Isquêmicas\".\n",
    "Isso ajuda a identificar padrões maiores, como: \"As doenças isquêmicas representam 40% das internações cardíacas\".\n",
    "4. Filtragem de Interesse\n",
    "Os arquivos do SIH costumam ser enormes (milhares de linhas).\n",
    "Nesta etapa, você descarta tudo o que não é cardíaco. Você filtra o banco de dados para manter apenas as linhas onde o código começa com a letra \"I\" (ou os intervalos específicos que vimos antes). Isso deixa o processamento muito mais rápido.\n",
    "5. Cruzamento com Desfechos (O \"Pulo do Gato\")\n",
    "O CID sozinho diz o que a pessoa tem, mas você precisa saber o que aconteceu:\n",
    "Mortalidade: Verifique a coluna de \"Motivo da Saída\" ou \"Óbito\". Se o motivo for óbito, você marca aquela linha.\n",
    "Tempo de permanência: Calcule a diferença entre a data de internação e a data de saída para ver quais doenças cardíacas seguram o paciente por mais tempo no hospital.\n",
    "Custos: Veja o valor total pago pela AIH para entender o impacto financeiro de cada doença.\n",
    "6. Verificação de Consistência\n",
    "Antes de finalizar, faça uma \"checagem de realidade\":\n",
    "Existem idades impossíveis (ex: 150 anos)?\n",
    "Existem diagnósticos de gravidez em homens?\n",
    "Existem diagnósticos cardíacos infantis em alas geriátricas?\n",
    "\n",
    "merge com CID10MORB.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f8f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9fff621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255793/2187270669.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df_cardio = df[df['LINHAA'].str.contains(r'^\\*I(0[0-9]|[1-4][0-9]|5[0-2])', na=False)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3         *I119\n",
       "6         *I219\n",
       "26        *I461\n",
       "28        *I461\n",
       "31        *I269\n",
       "          ...  \n",
       "334267    *I469\n",
       "334280    *I499\n",
       "334282    *I219\n",
       "334284    *I509\n",
       "334288    *I461\n",
       "Name: LINHAA, Length: 42313, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cardio = df[df['LINHAA'].str.contains(r'^\\*I(0[0-9]|[1-4][0-9]|5[0-2])', na=False)]\n",
    "\n",
    "df_cardio['LINHAA']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb97a2",
   "metadata": {},
   "source": [
    "### Descrição das Colunas do DataFrame\n",
    "\n",
    "Aqui está uma interpretação do significado de cada coluna no seu DataFrame `df`:\n",
    "\n",
    "*   **`ORIGEM`**: Indica a origem do registro (e.g., tipo de declaração de óbito).\n",
    "*   **`TIPOBITO`**: Tipo de óbito (e.g., fetal, não fetal).\n",
    "*   **`DTOBITO`**: Data do óbito.\n",
    "*   **`HORAOBITO`**: Hora do óbito.\n",
    "*   **`NATURAL`**: Naturalidade (código do município ou estado de nascimento).\n",
    "*   **`CODMUNNATU`**: Código do município de nascimento.\n",
    "*   **`DTNASC`**: Data de nascimento do falecido.\n",
    "*   **`IDADE`**: Idade do falecido (frequentemente codificada, e.g., em dias, meses, anos).\n",
    "*   **`SEXO`**: Sexo do falecido (e.g., 1 para masculino, 2 para feminino).\n",
    "*   **`RACACOR`**: Raça/Cor do falecido (e.g., branca, preta, parda, amarela, indígena).\n",
    "*   **`ESTCIV`**: Estado civil do falecido.\n",
    "*   **`ESC`**: Escolaridade do falecido (nível de instrução).\n",
    "*   **`ESC2010`**: Escolaridade do falecido conforme a classificação de 2010.\n",
    "*   **`SERIESCFAL`**: Série escolar do falecido (nível de ensino).\n",
    "*   **`OCUP`**: Ocupação do falecido.\n",
    "*   **`CODMUNRES`**: Código do município de residência do falecido.\n",
    "*   **`LOCOCOR`**: Local de ocorrência do óbito (e.g., hospital, domicílio, via pública).\n",
    "*   **`CODESTAB`**: Código do estabelecimento de saúde onde ocorreu o óbito (se aplicável).\n",
    "*   **`ESTABDESCR`**: Descrição do estabelecimento (Nome do Hospital, por exemplo).\n",
    "*   **`CODMUNOCOR`**: Código do município de ocorrência do óbito.\n",
    "*   **`IDADEMAE`**: Idade da mãe (para óbitos infantis ou maternos).\n",
    "*   **`ESCMAE`**: Escolaridade da mãe.\n",
    "*   **`ESCMAE2010`**: Escolaridade da mãe conforme a classificação de 2010.\n",
    "*   **`SERIESCMAE`**: Série escolar da mãe.\n",
    "*   **`OCUPMAE`**: Ocupação da mãe.\n",
    "*   **`QTDFILVIVO`**: Quantidade de filhos vivos (para mães).\n",
    "*   **`QTDFILMORT`**: Quantidade de filhos mortos (para mães).\n",
    "*   **`GRAVIDEZ`**: Status de gravidez (relacionado a óbitos maternos).\n",
    "*   **`SEMAGESTAC`**: Semanas de gestação (para óbitos fetais ou infantis).\n",
    "*   **`GESTACAO`**: Tipo de gestação.\n",
    "*   **`PARTO`**: Tipo de parto.\n",
    "*   **`OBITOPARTO`**: Se o óbito ocorreu durante o parto (sim/não).\n",
    "*   **`PESO`**: Peso ao nascer (para óbitos infantis).\n",
    "*   **`TPMORTEOCO`**: Tipo de morte na ocorrência.\n",
    "*   **`OBITOGRAV`**: Se o óbito foi relacionado à gravidez.\n",
    "*   **`OBITOPUERP`**: Se o óbito foi relacionado ao puerpério (período pós-parto).\n",
    "*   **`ASSISTMED`**: Se houve assistência médica no momento do óbito.\n",
    "*   **`EXAME`**: Se foi realizado algum exame (e.g., laboratorial).\n",
    "*   **`CIRURGIA`**: Se foi realizada alguma cirurgia (os valores 1, 2, 9 e NaN provavelmente significam 'Sim', 'Não', 'Ignorado' e 'Ausente', respectivamente).\n",
    "*   **`NECROPSIA`**: Se foi realizada necropsia.\n",
    "*   **`LINHAA`, `LINHAB`, `LINHAC`, `LINHAD`**: Causas de óbito listadas sequencialmente na Declaração de Óbito (parte I), representando a cadeia de eventos que levou à morte.\n",
    "*   **`LINHAII`**: Outras condições significativas que contribuíram para a morte, mas não faziam parte da cadeia direta de eventos.\n",
    "*   **`CAUSABAS`**: Causa básica do óbito (a doença ou lesão que iniciou a cadeia de eventos mórbidos).\n",
    "*   **`CB_PRE`**: Causa básica pré-codificada.\n",
    "*   **`COMUNSVOIM`**: Provavelmente alguma variável comum para o Sistema de Informações sobre Mortalidade (SIM).\n",
    "*   **`DTATESTADO`**: Data de atestado do óbito.\n",
    "*   **`CIRCOBITO`**: Circunstância do óbito (e.g., acidente, suicídio, homicídio).\n",
    "*   **`ACIDTRAB`**: Se foi acidente de trabalho.\n",
    "*   **`FONTE`**: Fonte da informação.\n",
    "*   **`NUMEROLOTE`**: Número do lote ao qual o registro pertence.\n",
    "*   **`TPPOS`**: Tipo de processamento ou posição.\n",
    "*   **`DTINVESTIG`**: Data da investigação do óbito (se aplicável).\n",
    "*   **`CAUSABAS_O`**: Causa básica original (antes de possíveis correções).\n",
    "*   **`DTCADASTRO`**: Data de cadastro do registro.\n",
    "*   **`ATESTANTE`**: Informações sobre o profissional que atestou o óbito.\n",
    "*   **`STCODIFICA`**: Status da codificação.\n",
    "*   **`CODIFICADO`**: Se o óbito foi codificado (sim/não).\n",
    "*   **`VERSAOSIST`**: Versão do sistema utilizado para o registro.\n",
    "*   **`VERSAOSCB`**: Versão da SCB (provavelmente da Classificação Brasileira de Ocupações ou similar).\n",
    "*   **`FONTEINV`**: Fonte da investigação.\n",
    "*   **`DTRECEBIM`**: Data de recebimento do registro.\n",
    "*   **`ATESTADO`**: Se o óbito foi atestado.\n",
    "*   **`DTRECORIGA`**: Data do registro original.\n",
    "*   **`CAUSAMAT`**: Causa materna do óbito.\n",
    "*   **`ESCMAEAGR1`**: Escolaridade da mãe agregada (nível mais amplo).\n",
    "*   **`ESCFALAGR1`**: Escolaridade do falecido agregada.\n",
    "*   **`STDOEPIDEM`**: Status de doença epidêmica.\n",
    "*   **`STDONOVA`**: Status de doença nova.\n",
    "*   **`DIFDATA`**: Diferença de datas (e.g., entre óbito e registro).\n",
    "*   **`NUDIASOBCO`**: Número de dias de observação de co-ocorrências.\n",
    "*   **`NUDIASOBIN`**: Número de dias de observação em investigação.\n",
    "*   **`DTCADINV`**: Data de cadastro da investigação.\n",
    "*   **`TPOBITOCOR`**: Tipo de ocorrência de óbito.\n",
    "*   **`DTCONINV`**: Data de conclusão da investigação.\n",
    "*   **`FONTES`**: Fontes adicionais.\n",
    "*   **`TPRESGINFO`**: Tipo de registro de informação.\n",
    "*   **`TPNIVELINV`**: Tipo de nível de investigação.\n",
    "*   **`NUDIASINF`**: Número de dias de internação ou doença.\n",
    "*   **`DTCADINF`**: Data de cadastro de informações.\n",
    "*   **`MORTEPARTO`**: Morte relacionada ao parto.\n",
    "*   **`DTCONCASO`**: Data de conclusão do caso.\n",
    "*   **`FONTESINF`**: Fontes de informação.\n",
    "*   **`ALTCAUSA`**: Causa alternativa.\n",
    "*   **`CONTADOR`**: Um contador ou identificador de registro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
